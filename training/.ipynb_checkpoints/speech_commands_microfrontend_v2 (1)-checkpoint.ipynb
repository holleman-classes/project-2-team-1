{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "undefined-familiar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:07:44.098850: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-04 21:07:44.128882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 21:07:44.571070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm # replace with this if moving out of notebook\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4253538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"CUDA is available\")\n",
    "\n",
    "    # Print the name of the available device\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    print(\"GPU device name:\", device_name)\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bearing-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incredible-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "i16min = -2**15\n",
    "i16max = 2**15-1\n",
    "fsamp = 16000\n",
    "wave_length_ms = 1000\n",
    "wave_length_samps = int(wave_length_ms*fsamp/1000)\n",
    "window_size_ms=60\n",
    "window_step_ms=40\n",
    "num_filters = 32\n",
    "use_microfrontend = True\n",
    "# dataset = 'mini-speech'\n",
    "# dataset = 'full-speech-ds' # use the full speech commands as a pre-built TF dataset \n",
    "dataset = 'full-speech-files' # use the full speech commands stored as files \n",
    "\n",
    "silence_str = \"_silence\"  \n",
    "unknown_str = \"_unknown\"\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-movement",
   "metadata": {},
   "source": [
    "Apply the frontend to an example signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rising-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = ['yes', 'computer'] \n",
    "if dataset == 'mini-speech':\n",
    "  data_dir = pathlib.Path(os.path.join(os.getenv(\"HOME\"), 'data/mini_speech_commands'))\n",
    "  if not data_dir.exists():\n",
    "    tf.keras.utils.get_file('mini_speech_commands.zip',\n",
    "          origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "          extract=True, cache_dir='.', cache_subdir='data')\n",
    "  # commands = np.array(tf.io.gfile.listdir(str(data_dir))) # if you want to use all the command words\n",
    "  # commands = commands[commands != 'README.md']\n",
    "elif dataset == 'full-speech-files':\n",
    "  # data_dir = '/dfs/org/Holleman-Coursework/data/speech_dataset'\n",
    "  data_dir = pathlib.Path(os.path.join('./custom-speech'))\n",
    "\n",
    "elif dataset == 'full-speech-ds':\n",
    "    raise RuntimeError(\"full-speech-ds is not really supported yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e571f98-2d10-4809-ae2e-abd8d202181a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('custom-speech')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precious-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_list: ['_silence', '_unknown', 'yes', 'computer']\n"
     ]
    }
   ],
   "source": [
    "label_list = commands.copy()\n",
    "label_list.insert(0, silence_str)\n",
    "label_list.insert(1, unknown_str)\n",
    "print('label_list:', label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08637a7-35d4-493c-b3c9-31ea358de82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/david/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "computer  noise\t\t    training_list.txt  validation_list.txt\r\n",
      "left\t  testing_list.txt  unrused_noise_     yes\r\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprising-adjustment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total examples: 11851\n",
      "Example file tensor: tf.Tensor(b'custom-speech/yes/01bb6a2a_nohash_1.wav', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:07:47.342423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.363382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.363535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.364513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.364630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.364713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.836033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.836206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.836306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 21:07:47.836391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'mini-speech' or dataset == 'full-speech-files':\n",
    "    filenames = tf.io.gfile.glob(str(data_dir) + '/*/*.wav') \n",
    "    # with the next commented-out line, you can choose only files for words in label_list\n",
    "    # filenames = tf.concat([tf.io.gfile.glob(str(data_dir) + '/' + cmd + '/*') for cmd in label_list], 0)\n",
    "    filenames = tf.random.shuffle(filenames)\n",
    "    num_samples = len(filenames)\n",
    "    print('Number of total examples:', num_samples)\n",
    "    # print('Number of examples per label:',\n",
    "    #       len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
    "    print('Example file tensor:', filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "printable-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full-speech-files is in progress.  Good luck!\n",
      "Training set size 8297\n",
      "Validation set size 1185\n",
      "Test set size 2369\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'mini-speech':\n",
    "  print('Using mini-speech')\n",
    "  num_train_files = int(0.8*num_samples) \n",
    "  num_val_files = int(0.1*num_samples) \n",
    "  num_test_files = num_samples - num_train_files - num_val_files\n",
    "  train_files = filenames[:num_train_files]\n",
    "  val_files = filenames[num_train_files: num_train_files + num_val_files]\n",
    "  test_files = filenames[-num_test_files:]\n",
    "elif dataset == 'full-speech-files':  \n",
    "  # the full speech-commands set lists which files are to be used\n",
    "  # as test and validation data; train with everything else\n",
    "  fname_val_files = os.path.join(data_dir, 'validation_list.txt')    \n",
    "  with open(fname_val_files) as fpi_val:\n",
    "    val_files = fpi_val.read().splitlines()\n",
    "  # validation_list.txt only lists partial paths\n",
    "  val_files = [os.path.join(data_dir, fn) for fn in val_files]\n",
    "  fname_test_files = os.path.join(data_dir, 'testing_list.txt')\n",
    "\n",
    "  with open(fname_test_files) as fpi_tst:\n",
    "    test_files = fpi_tst.read().splitlines()\n",
    "  # testing_list.txt only lists partial paths\n",
    "  test_files = [os.path.join(data_dir, fn).rstrip() for fn in test_files]    \n",
    "\n",
    "  # convert the TF tensor filenames into an array of strings so we can use basic python constructs\n",
    "  train_files = [f.decode('utf8') for f in filenames.numpy()]\n",
    "  # don't train with the _background_noise_ files; exclude when directory name starts with '_'\n",
    "  train_files = [f for f in train_files if f.split('/')[-2][0] != '_']\n",
    "  # validation and test files are listed explicitly in *_list.txt; train with everything else\n",
    "  train_files = list(set(train_files) - set(test_files) - set(val_files))\n",
    "  # now convert back into a TF tensor so we can use the tf.dataset pipeline\n",
    "  train_files = tf.constant(train_files)    \n",
    "  print(\"full-speech-files is in progress.  Good luck!\")\n",
    "elif dataset == 'full-speech-ds':  \n",
    "    print(\"Using full-speech-ds. This is in progress.  Good luck!\")\n",
    "else:\n",
    "  raise ValueError(\"dataset must be either full-speech-files, full-speech-ds or mini-speech\")\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "connected-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  in_set = tf.reduce_any(parts[-2] == label_list)\n",
    "  label = tf.cond(in_set, lambda: parts[-2], lambda: tf.constant(unknown_str))\n",
    "  # print(f\"parts[-2] = {parts[-2]}, in_set = {in_set}, label = {label}\")\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
    "  # to work in a TensorFlow graph.\n",
    "  return  label # parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "false-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "injured-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length (16000 samples)\n",
    "  zero_padding = tf.zeros([wave_length_samps] - tf.shape(waveform), dtype=tf.int16)\n",
    "  waveform = tf.cast(0.5*waveform*(i16max-i16min), tf.int16)  # scale float [-1,+1]=>INT16\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  ## Make sure these labels correspond to those used in micro_features_micro_features_generator.cpp\n",
    "  spectrogram = frontend_op.audio_microfrontend(equal_length, sample_rate=fsamp, num_channels=num_filters,\n",
    "                                    window_size=window_size_ms, window_step=window_step_ms)\n",
    "  return spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-newark",
   "metadata": {},
   "source": [
    "Function to convert each waveform in a set into a spectrogram, then convert those\n",
    "back into a dataset using `from_tensor_slices`.  (We should be able to use \n",
    "`wav_ds.map(get_spectrogram_and_label_id)`, but there is a problem with that process).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "normal-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silence_dataset(num_waves, samples_per_wave, rms_noise_range=[0.01,0.2], silent_label=silence_str):\n",
    "    # create num_waves waveforms of white gaussian noise, with rms level drawn from rms_noise_range\n",
    "    # to act as the \"silence\" dataset\n",
    "    rng = np.random.default_rng()\n",
    "    rms_noise_levels = rng.uniform(low=rms_noise_range[0], high=rms_noise_range[1], size=num_waves)\n",
    "    rand_waves = np.zeros((num_waves, samples_per_wave), dtype=np.float32) # pre-allocate memory\n",
    "    for i in range(num_waves):\n",
    "        rand_waves[i,:] = rms_noise_levels[i]*rng.standard_normal(samples_per_wave)\n",
    "    labels = [silent_label]*num_waves\n",
    "    return tf.data.Dataset.from_tensor_slices((rand_waves, labels))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "awful-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavds2specds(waveform_ds, verbose=True):\n",
    "  wav, label = next(waveform_ds.as_numpy_iterator())\n",
    "  one_spec = get_spectrogram(wav)\n",
    "  one_spec = tf.expand_dims(one_spec, axis=0)  # add a 'batch' dimension at the front\n",
    "  one_spec = tf.expand_dims(one_spec, axis=-1) # add a singleton 'channel' dimension at the back    \n",
    "\n",
    "  num_waves = 0 # count the waveforms so we can allocate the memory\n",
    "  for wav, label in waveform_ds:\n",
    "    num_waves += 1\n",
    "  print(f\"About to create spectrograms from {num_waves} waves\")\n",
    "  spec_shape = (num_waves,) + one_spec.shape[1:] \n",
    "  spec_grams = np.nan * np.zeros(spec_shape)  # allocate memory\n",
    "  labels = np.nan * np.zeros(num_waves)\n",
    "  idx = 0\n",
    "  for wav, label in waveform_ds:    \n",
    "    if verbose and idx % 250 == 0:\n",
    "      print(f\"\\r {idx} wavs processed\", end='')\n",
    "    spectrogram = get_spectrogram(wav)\n",
    "    # TF conv layer expect inputs structured as 4D (batch_size, height, width, channels)\n",
    "    # the microfrontend returns 2D tensors (freq, time), so we need to \n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=0)  # add a 'batch' dimension at the front\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=-1) # add a singleton 'channel' dimension at the back\n",
    "    spec_grams[idx, ...] = spectrogram\n",
    "    new_label = label.numpy().decode('utf8')\n",
    "    new_label_id = np.argmax(new_label == np.array(label_list))    \n",
    "    labels[idx] = new_label_id # for numeric labels\n",
    "    # labels.append(new_label) # for string labels\n",
    "    idx += 1\n",
    "  labels = np.array(labels, dtype=int)\n",
    "  output_ds = tf.data.Dataset.from_tensor_slices((spec_grams, labels))  \n",
    "  return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appropriate-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:07:48.108993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [8297]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to create spectrograms from 8297 waves\n",
      "\r",
      " 0 wavs processed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 21:07:49.038919: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at constant_op.cc:171 : INVALID_ARGUMENT: Dimension -1506930 must be >= 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} Dimension -1506930 must be >= 0 [Op:Fill]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m files_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(train_files)\n\u001b[1;32m      4\u001b[0m waveform_ds \u001b[38;5;241m=\u001b[39m files_ds\u001b[38;5;241m.\u001b[39mmap(get_waveform_and_label, num_parallel_calls\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n\u001b[0;32m----> 5\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mwavds2specds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mwavds2specds\u001b[0;34m(waveform_ds, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m250\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m wavs processed\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mget_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TF conv layer expect inputs structured as 4D (batch_size, height, width, channels)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# the microfrontend returns 2D tensors (freq, time), so we need to \u001b[39;00m\n\u001b[1;32m     21\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(spectrogram, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# add a 'batch' dimension at the front\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mget_spectrogram\u001b[0;34m(waveform)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_spectrogram\u001b[39m(waveform):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;66;03m# Concatenate audio with padding so that all audio clips will be of the \u001b[39;00m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;66;03m# same length (16000 samples)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m   zero_padding \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwave_length_samps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   waveform \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mwaveform\u001b[38;5;241m*\u001b[39m(i16max\u001b[38;5;241m-\u001b[39mi16min), tf\u001b[38;5;241m.\u001b[39mint16)  \u001b[38;5;66;03m# scale float [-1,+1]=>INT16\u001b[39;00m\n\u001b[1;32m      6\u001b[0m   equal_length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([waveform, zero_padding], \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} Dimension -1506930 must be >= 0 [Op:Fill]"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "num_train_files = len(train_files)\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = wavds2specds(waveform_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-consideration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(audio.numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  ax.set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  # transpose so that the time is\n",
    "  # represented in the x-axis (columns).\n",
    "  freq_bins = spectrogram.shape[1]\n",
    "  time_dur = spectrogram.shape[0]\n",
    "  X = np.arange(time_dur)\n",
    "  Y = range(freq_bins)\n",
    "  ax.pcolormesh(X, Y, spectrogram.T)\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa4527-72ed-45fc-8c06-9333274fce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "for i, (spectrogram, label_id) in enumerate(train_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
    "  ax.set_title(label_list[np.int(label_id)])\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fc7a4-2a9b-45d9-9a0a-6561afd7b828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_with_noise(ds_input, rms_level=0.25):\n",
    "  rng = tf.random.Generator.from_seed(1234)\n",
    "  wave_shape = tf.constant((wave_length_samps,))\n",
    "  def add_noise(waveform, label):\n",
    "    noise = rms_level*rng.normal(shape=wave_shape)\n",
    "    zero_padding = tf.zeros([wave_length_samps] - tf.shape(waveform), dtype=tf.float32)\n",
    "    waveform = tf.concat([waveform, zero_padding], 0)    \n",
    "    noisy_wave = waveform + noise\n",
    "    return noisy_wave, label\n",
    "\n",
    "  return ds_input.map(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform_ds = augment_with_noise(waveform_ds)\n",
    "count = 0\n",
    "for w,l in waveform_ds:\n",
    "  if w.shape != (16000,):\n",
    "    print(f\"element {count} has shape {w.shape}\")\n",
    "    break\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_16000(waveform, label):\n",
    "    zero_padding = tf.zeros([wave_length_samps] - tf.shape(waveform), dtype=tf.float32)\n",
    "    waveform = tf.concat([waveform, zero_padding], 0)        \n",
    "    return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(dataset):\n",
    "    counts = {}\n",
    "    for _, lbl in dataset:\n",
    "        if lbl.dtype == tf.string:\n",
    "            label = lbl.numpy().decode('utf-8')\n",
    "        else:\n",
    "            label = lbl.numpy()\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect what we did to generate the training dataset into a \n",
    "# function, so we can repeat with the validation and test sets.\n",
    "def preprocess_dataset(files, num_silent=None, noisy_reps_of_known=None):\n",
    "  # if noisy_reps_of_known is not None, it should be a list of rms noise levels\n",
    "  # For every target word in the data set, 1 copy will be created with each level \n",
    "  # of noise added to it.  So [0.1, 0.2] will add 2x noisy copies of the target words \n",
    "  if num_silent is None:\n",
    "    num_silent = int(0.2*len(files))+1\n",
    "  print(f\"Processing {len(files)} files\")\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  waveform_ds = files_ds.map(get_waveform_and_label)\n",
    "  if noisy_reps_of_known is not None:\n",
    "    # create a few copies of only the target words to balance the distribution\n",
    "    # create a tmp dataset with only the target words\n",
    "    ds_only_cmds = waveform_ds.filter(lambda w,l: tf.reduce_any(l == commands))\n",
    "    for noise_level in noisy_reps_of_known:\n",
    "       waveform_ds = waveform_ds.concatenate(copy_with_noise(ds_only_cmds, rms_level=noise_level))\n",
    "  if num_silent > 0:\n",
    "    silent_wave_ds = create_silence_dataset(num_silent, wave_length_samps, \n",
    "                                            rms_noise_range=[0.01,0.2], \n",
    "                                            silent_label=silence_str)\n",
    "    waveform_ds = waveform_ds.concatenate(silent_wave_ds)\n",
    "  print(f\"Added {num_silent} silent wavs and ?? noisy wavs\")\n",
    "  num_waves = 0\n",
    "  output_ds = wavds2specds(waveform_ds)\n",
    "  return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(train_files)}/{len(val_files)}/{len(test_files)} training/validation/test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_files[:20])\n",
    "print(label_list)\n",
    "train_files[:20]\n",
    "\n",
    "tmp_ds = preprocess_dataset(train_files[:20])\n",
    "print(count_labels(tmp_ds))\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    device_name = '/GPU:0'\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    device_name = '/CPU:0'\n",
    "    print('Running on CPU')\n",
    "\n",
    "with tf.device(device_name):\n",
    "    tmp_ds = preprocess_dataset(train_files[:20], noisy_reps_of_known=[0.05,0.1])\n",
    "    print(count_labels(tmp_ds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    device_name = '/GPU:0'\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    device_name = '/CPU:0'\n",
    "    print('Running on CPU')\n",
    "\n",
    "with tf.device(device_name):\n",
    "    train_ds = preprocess_dataset(train_files, noisy_reps_of_known=[0.05,0.1,0.15,0.2,0.25])\n",
    "    val_ds = preprocess_dataset(val_files)\n",
    "    test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training data set\")\n",
    "print(count_labels(train_ds))\n",
    "print(\"val_ds data set\")\n",
    "print(count_labels(val_ds))\n",
    "print(\"test_ds data set\")\n",
    "print(count_labels(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(int(len(train_files)*1.2))\n",
    "val_ds = val_ds.shuffle(int(len(val_files)*1.2))\n",
    "test_ds = test_ds.shuffle(int(len(test_files)*1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in train_ds.take(1):\n",
    "  spec1 = spectrogram\n",
    "# take(1) takes 1 *batch*, so we have to select the first \n",
    "# spectrogram from it, hence the [0]\n",
    "print(f\"Spectrogram shape {spec1[0].shape}\")\n",
    "print(f\"ranges from {np.min(spec1)} to {np.max(spec1)}\")   # min/max across the whole batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in train_ds.take(1):\n",
    "  # take(1) takes 1 *batch*, so we have to select the first \n",
    "  # spectrogram from it, hence the [0]\n",
    "  input_shape = spectrogram[0].shape  \n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', input_shape)\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, DepthwiseConv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def build_model(l2, lr):\n",
    "    l2_value=l2\n",
    "    model = models.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv2D(32, kernel_size=(3,3),strides=(2,2), activation='relu', kernel_regularizer=regularizers.l2(l2_value),name=\"conv1\"),\n",
    "        BatchNormalization(name='batch1'),\n",
    "        MaxPooling2D(name='pool1'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(l2_value),name='depthconv1'),\n",
    "        Conv2D(20, 1, activation='relu', kernel_regularizer=regularizers.l2(l2_value),name='conv2'),\n",
    "        MaxPooling2D(pool_size=(4,4),name='pool2'),\n",
    "        BatchNormalization(name='batch2'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_value)),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_value)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_labels),\n",
    "    ], name=\"simple_cnn\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.01,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "    return model, early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_value=0.01\n",
    "lr=0.001\n",
    "EPOCHS=25\n",
    "model, early_stopping = build_model(l2_value,lr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = f\"./models/model.h5\" \n",
    "print(f\"Saving model to {model_file_name}\")\n",
    "model.save(model_file_name, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157289b7-b7c9-469a-909a-4d346e583a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_file_name.split('.')[0] + '.txt', 'w') as fpo:\n",
    "    fpo.write(f\"i16min            = {i16min           }\\n\")\n",
    "    fpo.write(f\"i16max            = {i16max           }\\n\")\n",
    "    fpo.write(f\"fsamp             = {fsamp            }\\n\")\n",
    "    fpo.write(f\"wave_length_ms    = {wave_length_ms   }\\n\")\n",
    "    fpo.write(f\"wave_length_samps = {wave_length_samps}\\n\")\n",
    "    fpo.write(f\"window_size_ms    = {window_size_ms   }\\n\")\n",
    "    fpo.write(f\"window_step_ms    = {window_step_ms   }\\n\")\n",
    "    fpo.write(f\"num_filters       = {num_filters      }\\n\")\n",
    "    fpo.write(f\"use_microfrontend = {use_microfrontend}\\n\")\n",
    "    fpo.write(f\"label_list        = {label_list}\\n\")\n",
    "    fpo.write(f\"spectrogram_shape = {spectrogram.numpy().shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.subplot(2,1,1)\n",
    "plt.semilogy(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-coverage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-victory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset = train_ds.unbatch()\n",
    "print(\"On training set:\")\n",
    "\n",
    "ds_audio = []\n",
    "ds_labels = []\n",
    "\n",
    "for audio, label in dset:\n",
    "  ds_audio.append(audio.numpy())\n",
    "  ds_labels.append(label.numpy())\n",
    "\n",
    "ds_labels = np.array(ds_labels)\n",
    "ds_audio = np.array(ds_audio)\n",
    "\n",
    "model_out = model.predict(ds_audio)\n",
    "y_pred = np.argmax(model_out, axis=1)\n",
    "y_true = ds_labels\n",
    "\n",
    "ds_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Data set accuracy: {ds_acc:.0%}')\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-stopping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_files = [data_dir/'computer/computer_171206.wav', \n",
    "                data_dir/'yes/0a2b400e_nohash_0.wav',\n",
    "                data_dir/'left/0a2b400e_nohash_0.wav']\n",
    "fstr_list = [str(f) for f in sample_files]\n",
    "sample_ds = preprocess_dataset(fstr_list, num_silent=1)\n",
    "count = 1\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "  prediction = model(spectrogram)\n",
    "  plt.subplot(len(sample_files)+1, 1, count)\n",
    "  plt.bar(label_list, tf.nn.softmax(prediction[0]))\n",
    "  plt.title(f'Predictions for \"{label_list[label[0]]}\"')\n",
    "  plt.show()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-preserve",
   "metadata": {},
   "source": [
    "## Quantize and Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calibration_steps = 10\n",
    "ds_iter = val_ds.unbatch().batch(1).as_numpy_iterator()\n",
    "def representative_dataset_gen():\n",
    "  for _ in range(num_calibration_steps):\n",
    "    next_input = next(ds_iter)[0]\n",
    "    next_input = next_input.astype(np.float32)  # (DIFF_FROM_LECTURE)\n",
    "    yield [next_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8; should match dat_q in eval_quantized_model.py\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './models/model.tflite'\n",
    "with open(fname, \"wb\") as fpo:\n",
    "  num_bytes_written = fpo.write(tflite_quant_model)\n",
    "print(f\"Wrote {num_bytes_written} / {len(tflite_quant_model)} bytes to tflite file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i ./models/model.tflite >! ./models/model.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the C source file\n",
    "!cat {'./models/model.cc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(model.input.shape[2]) + ' kFeatureSliceSize') #kFeatureSliceSize\n",
    "print(str(model.input.shape[1]) + ' kFeatureSliceCount') #kFeatureSliceCount\n",
    "print(str(window_step_ms) + ' kFeatureSliceStrideMs') #kFeatureSliceStrideMs\n",
    "print(str(window_size_ms) + ' kFeatureSliceDurationMs') #kFeatureSliceDurationMs\n",
    "print(str((window_size_ms * fsamp / 1000)) + ' kMaxAudioSampleSize') #kMaxAudioSampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf5603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
